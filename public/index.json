[{"categories":null,"contents":" The Kidney Donation Problem Some of the most powerful research coming from Operations Researchers originates in the healthcare field where there are some notorious inefficiencies. If you\u0026rsquo;ve ever had a loved one in need of a life-saving transplant, this emerging research may one day become the difference between life and death for many.\nTo preface the subject, kidney exchanges aren\u0026rsquo;t necessarily a new idea, we\u0026rsquo;ve just never found a way to do them at scale. This project is in inspiration of this PNAS research article.\nFull disclosure, I do not have a biology degree, that being said I can at least offer some information on the process of organ donation. For kidneys specifically, humans can live with only one, thus many healthy family members will step up and offer one of their\u0026rsquo;s to a related family member in need of one. The issue is, there must be a match or some compatibility in blood type from donor to patient. Without it, we run the risk of the kidney being rejected by the patient. This presents a special sort of challenge. What if a patient doesn\u0026rsquo;t have a family member with a compatible kidney? It turns out this is quite likely (25% of siblings are \u0026ldquo;exact matches\u0026rdquo;1). Usually what happens when there isn\u0026rsquo;t a family member willing to give a kidney is that patients are put into a pool along with everyone else who doesn\u0026rsquo;t have a matching family donor. Think of this as a sort of waiting list. From here, they wait for an unrelated organ donor (generally upon death) to give them a kidney, which may or may not ever happen. Thus we introduce the kidney exchange.\nKidney Exchanges To solve this predicament, we can introduce an exchange system. The way this works is in a sort of \u0026ldquo;I\u0026rsquo;ll scratch your back, if you scratch mine\u0026rdquo; type solution. The idea is this: we have a patient; this patient has a family member willing to give a kidney, but unable to due to incompatibility; we find another patient who is in a similar situation; we match these patient-donor pairs with each other, giving us four total people in the exchange; the donors each give a kidney to the patient in which they are not family related. The power of this approach, however, is that we increase the supply of kidneys in the system. Of course if you\u0026rsquo;re following along, this also presents a challenge of higher magnitude because now we must find a reciprocal match as opposed to a one-to-one match. Thus, we implement chains and cycles.\nChains The idea behind chains is that with one Non-directed Donor (someone who gives their kidney altruistically or in posthumous organ donation), we can give that kidney to the first person on our kidney pool/list, and then the incompatible family member who would have liked to give them a kidney will give theirs to another patient in need. This starts a chain reaction of patients getting kidneys and donors giving theirs to those they don\u0026rsquo;t know, in exchange for their family member receiving a kidney from someone else. Chains look something like this:\nCycles Cycles are similar to chains in their structure, the only difference is that cycles aren\u0026rsquo;t started by NDDs but rather a donor from a patient-donor pairing. Ultimately, cycles end with a donor giving up their kidney to the patient whose donor began the cycle. This looks something like this:\nAs we\u0026rsquo;ll see later, we will want to limit cycles to a maximum length of five. There are a few reasons we do this, but it is mainly because the longer a cycle is, the longer the time in which something could happen such that the patient-donor pair who began the cycle doesn\u0026rsquo;t get a kidney at the end of the cycle. Restated from above, a cycle begins when a donor from an incompatible pairing gives their kidney to someone whom they do not know, but are compatible with. The cycle completes when a donor gives their kidney to the patient who\u0026rsquo;s donor began the cycle.\nPerhaps you\u0026rsquo;re wondering why have cycles at all? The reason we allow for cycles is because it gives our optimization problem some flexibility in which we can increase the number of patients who get compatible kidneys, and the kidneys in which they receive are more compatible. If we keep them short, we can give ourselves more opportunity (increase our feasible region) to match patients well.\nThe Data The kidney exchange problem, in simplified terms, is an integer-programming optimization problem- nothing more. We are given a dataset of pairs in which the patient and donor are not compatible, and a score for which each pair is compatible with another pair (meaning there is a good match between the donor of one pair, and the patient of another pair). This compatibility score is the weights on our edges. The dataset looks like this:\nfrom to w ndd 0 721 38 13 1 272 5 978 2 71 44 999 2 411 58 \u0026hellip; \u0026hellip; \u0026hellip; As you can see, some pairs can perform more than one match- of course only one of these edges can have flow on it as once a patient has the kidney they need they won\u0026rsquo;t need another. Additionally, we should note that we are supplied with our NDDs of which there are only three. This tells us that we will only have three chains as chains can only be began from an NDD.\nTo explain the \u0026ldquo;from\u0026rdquo; and \u0026ldquo;to\u0026rdquo; columns, take the first row for example. Pairing 0\u0026rsquo;s donor can give their kidney to pairing 721\u0026rsquo;s patient in need of a kidney. This relationship may not be reciprocal, however. Meaning, 721\u0026rsquo;s donor may not be able to give their kidney to pairing 0\u0026rsquo;s patient. If 721\u0026rsquo;s donor did have a compatible kidney, there would be a row in which 721 is in the \u0026ldquo;from\u0026rdquo; column and 0 is in the \u0026ldquo;to\u0026rdquo; column, but for the sake of keeping the table simple I won\u0026rsquo;t show all the data. Rest assured, there are pairings in which pairing 0\u0026rsquo;s patient can receive a compatible kidney.\nFinally, w is subscripted at ij. For example, w0,721 is equal to 38.\nWe load the data into Julia like this:\n1 2 3 4 5 6 7 8 9 10 11 12 dat = readdlm(\u0026#34;donor-pool1.csv\u0026#34;, \u0026#39;,\u0026#39;, \u0026#39;\\n\u0026#39;, comments=true) fr = dat[:,1] to = dat[:,2] w = dat[:,3] N = dat[:,4] N = N[1:3,] V = union(fr,to) # set of all nodes E = collect(zip(fr,to)) # set of all edges W = Dict( (i,j) =\u0026gt; k for (i,j,k) in zip(fr,to,w) ) # weights on edges The Formulation For more detail, view the formulation in the PNAS article.\nDecision Variables In the kidney exchange problem, we want to maximize the compatibility between all patients and their newly matched donors. We achieve this through mathematical optimization which seeks to maximize our objective (patients treated and the corresponding compatibility) while remaining within constraints which we shall outline below. To do this, our optimizer will determine the pairs that should match with each other, which we will define as a decision variable.\nIf you\u0026rsquo;ve ever done an optimization problem, this may seem somewhat familiar. As with any optimization problem, we define our decision variables first. In terms of our problem, decision variables are the rows in which a pair in the \u0026ldquo;from\u0026rdquo; column should give its kidney to the patient in the \u0026ldquo;to\u0026rdquo; column. We can define this variable as y.\nyij is a binary variable. If a donor in the \u0026ldquo;from\u0026rdquo; pairing gives their kidney to a patient in the \u0026ldquo;to\u0026rdquo; pairing, this variable is set to 1. For example, if pair 0\u0026rsquo;s donor gives their kidney to pair 721\u0026rsquo;s patient, y0, 721 will be 1. The i and j represent the \u0026ldquo;from\u0026rdquo; and \u0026ldquo;to\u0026rdquo; pairs that are involved in the transplant. We also define a couple additional variables to set flow in and flow out. We will use these variables to limit the number of kidneys leaving a pair (donor gives their kidney) and the number of kidneys entering a pair (patient receives a kidney). We will set some constraints such that neither of these variables are greater than 1, but also such that if a donor gives their kidney, the incompatible patient whom they are giving the kidney for will eventually receive a compatible kidney from someone else. We can call these variables fo and fi. Each of these variables are subscripted with v. v is the set of our pairings, essentially a list from 0 to 1000 (including our NDDs). This way we can limit the flow in and out of each pairing in our dataset. In Julia\u0026rsquo;s JuMP, we can create a model and add constraints like this:\n1 2 3 4 5 6 7 8 m = Model() set_optimizer(m, Cbc.Optimizer) @variable(m, y[(i,j) in E], Bin) @variable(m, f_in[i in V]) @variable(m, f_out[i in V]) We\u0026rsquo;ll use the Cbc optimizer for this problem, of course there are others.\nObjective Function To solve, we want to maximize the number of pairings that receive a kidney and the corresponding compatibility (we\u0026rsquo;d much rather match a donor to a patient with higher compatibility). In words, we maximize the sum of our weights for which a \u0026ldquo;from\u0026rdquo; pair\u0026rsquo;s donor gives a kidney to a \u0026ldquo;to\u0026rdquo; pair\u0026rsquo;s patient. To do this, we multiply the weight by the corresponding yi,j decision variable.\n1 @objective(m, Max, sum( W[(i,j)]*y[(i,j)] for (i,j) in E )) Constraints We have six sets of constraints. In words, these constraints are:\nWe must first create two set-constraints that connect the value of our yij decision variable to our flow in and flow out decision variables (fo and fi). We need to constrain our flow in and flow out variables such that if a donor from a pairing gives a kidney, then their pairing (patient) will also receive a kidney. We should make both of these less than or equal to one. This constraint should be over the set of non-compatible pairs. Flow out of our NDDs should be less than or equal to one, as each NDD only has one kidney to give. The final constraint, or more accurately set of constraints, is to keep cycles from being greater than 5.\nRecursive Algorithm If we were to merely want a solution of chains and cycles to match donors and patients with kidneys, then we\u0026rsquo;ve completed the setup. However, we want a more realistic solution and to do that we have to constrain the length of cycles. In short, we can achieve this by solving the initial optimization problem, determining what cycles are longer than 5, and adding constraints to prevent each of those cycles when we resolve.\nIn words, the constraint to prevent a cycle of length greater than 5 can be modeled through a sum of the yij\u0026rsquo;s, for which yij is in the cycle that is longer than 5. We make this sum less than or equal to the length of the cycle, minus 1. In Julia, we can model it like so:\n1 @constraint(m, sum(y[(i,j)] for i in Cycle[z], j in Cycle[z] if in((i,j),E)) \u0026lt;= length(Cycle[z]) - 1) In the algorithm, we capture all cycles greater than 5 in an array called Cycle, and we create a constraint for each cycle over 5. Step-by-step, our algorithm works like this:\nSolve optimization problem Identify cycles over 5 Add constraints to prevent identified cycles over 5 Resolve We repeat 2-4 until we have an optimal solution with cycles all under 5. We\u0026rsquo;ve already shown how to build the initial optimization problem in Julia, so now we must show how to implement code to identify cycles, check if they\u0026rsquo;re over 5, add constraints, and resolve until we have a solution without cycles over 5.\nKEP in Julia To solve initially, we call JuMP\u0026rsquo;s optimize!() function like this:\n1 JuMP.optimize!(m) We\u0026rsquo;ll put this and everything that follows in a while loop- while all cycles aren\u0026rsquo;t under 5.\nIdentifying Cycles For speed in our algorithm, we should capture the yij values that are equal to 1, meaning that a donor and patient have been matched. This represents our solution set.\n1 2 3 4 5 6 set = [] for (i,j) in E if value.(y[(i,j)]) \u0026gt; 0 set = union(set,i,j) end end Then we instantiate a few arrays for use in our cycle identification:\n1 2 3 searched = [] # set of pairs we\u0026#39;ve searched U = [] # capture current cycle being identified ExtendU = set[rand(1:end)] # choose a node at random to start at We use searched to reduce the amount of pairs we search through each time, making our algorithm faster. Additionally, we use U to capture a single cycle at a time. With ExtendU we search for and assign the next matching pair to ExtendU, and then we add ExtendU to U. Once we\u0026rsquo;ve reached the end of the cycle (when ExtendU is equal to the beginning of U) and the length of the cycle is over 5, we add it to an array called Cycle.\nTo begin our search of mapping each cycle, which we do through another while loop- while all pairs in our solution haven\u0026rsquo;t been matched, we randomly start somewhere within our solution set. Then, we add ExtendU to U, and reassign ExtendU to nothing.\n1 2 3 while length(setdiff(set,searched))!=0 push!(U,ExtendU) ExtendU = nothing To perform our search of the next matching pair, we must introduce a for-loop to search through our solution set for the next pair. With an if statement, if we find the next pair we assign the pair to ExtendU.\n1 2 3 4 5 6 7 for j in union(U[1],setdiff(set,searched)) if (in((U[end],j),E) \u0026amp;\u0026amp; JuMP.value(y[(U[end],j)]) \u0026gt; 0) ExtendU = j push!(searched,ExtendU) break end end There are three things that can happen after this search:\nWe find the next pair and search for the next matching pair (while we haven\u0026rsquo;t matched the entire solution set). We don\u0026rsquo;t find the next matching pair. This means we\u0026rsquo;ve come to the end of a chain, because chains end with the final pair not matching with anyone (meaning their donor doesn\u0026rsquo;t give a kidney to anyone). If this happens, ExtendU will stay as nothing, so we\u0026rsquo;ll reset everything to search for cycle. There are other ways to do this, as we could start from our NDDs and find the chains and remove them from our search. This is another way in which we find many sub-chains through the way we shorten our searchable set. 1 2 3 4 5 6 7 8 9 # if we\u0026#39;ve found the end of a chain if isnothing(ExtendU) print(\u0026#34;Chain Found\u0026#34;,\u0026#34;\\n\u0026#34;) U = [] rn = setdiff(set,searched) ExtendU = rn[rand(1:end)] # don\u0026#39;t pick the same one twice.. push!(searched,ExtendU) # we\u0026#39;ve found a chain! end We find the next matching pair, but it\u0026rsquo;s the same as the pair for which our cycle began, meaning we\u0026rsquo;ve found a complete cycle. If this occurs, we check if the cycle is over 5. If it is, we add it to our Cycle array to add a constraint later on to prevent it from being in our next solution. If it\u0026rsquo;s under 5, we just reset and search for the next cycle. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # if we\u0026#39;ve found the end of a cycle if U[1]==ExtendU print(\u0026#34;Cycle Found\u0026#34;,\u0026#34;\\n\u0026#34;) # we\u0026#39;ve found a cycle, lets add it to our cycle list and create # a constraint, if necessary if length(U) \u0026gt; 5 push!(Cycle,U) cycles_over_five = cycles_over_five + 1 end U = [] rn = setdiff(set,searched) ExtendU = rn[rand(1:end)] push!(searched,ExtendU) end This will repeat until every pair in our solution has been matched, whether as a part of a chain or cycle.\nNotice we introduce a variable cycles_over_five. This variable will allow us to determine if our current solution has no cycles over 5. If that is the case, we\u0026rsquo;ll end the algorithm.\nAdd Constraints for Cycles over 5 We introduce a for-loop to add constraints for each of the cycles we identify in our solutions (remember we solve iteratively). The number of constraints will accumulate as we solve each time as Cycle is a global variable.\n1 2 3 4 for z in 1:length(Cycle) print(\u0026#34;constraint added\u0026#34;) @constraint(m, sum(y[(i,j)] for i in Cycle[z], j in Cycle[z] if in((i,j),E)) \u0026lt;= length(Cycle[z]) - 1) end Ending the Recursive Algorithm To end the algorithm, we check if the aforementioned cycles_over_five is 0, meaning we have a solution with no cycles over 5.\n1 2 3 4 5 6 7 8 9 if cycles_over_five==0 JuMP.optimize!(m) for (i,j) in E if value.(y[(i,j)]) \u0026gt; 0 println(i, \u0026#34; to \u0026#34;, j) end end break end To finish, we print our final solution and we break the while loop so that we stop iterating.\nSolution in a Readable Format To finish this project, we should put the final solution in a readable format for healthcare professionals to use the solution. When our final solution prints above, we have a list in the same order of our dataset showing which pairs should be matched. Ultimately, we want to see these in order. This means if pair 1 gives a kidney to pair 2, we next want to see who pair 2 gives their kidney to. To do this, we introduce some similar code to above to identify chains (starting from the NDD) and cycles.\nChains To find each chain, we capture our NDDs in a set, and we run a for-loop through each to match each pair that is in the chain having begun at an NDD. We supply the function with our solution from the recursive algorithm (y), our set of edges E, a list of pairs in our solution soln, and a list of our NDDs for this problem.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 function find_chains(y,E,soln,ndd) Chain = [] for i in ndd stop=false searched = [] U = [] ExtendU = i # start at ndd to find chain push!(searched,ExtendU) while stop==false # convert solution to a usable format for health professionals push!(U,ExtendU) ExtendU = nothing for j in union(U[1],setdiff(soln,searched)) if (in((U[end],j),E) \u0026amp;\u0026amp; y[(U[end],j)] \u0026gt; 0) ExtendU = j print(length(searched),\u0026#34;\\n\u0026#34;) push!(searched,ExtendU) break end end if isnothing(ExtendU) print(\u0026#34;Chain Found\u0026#34;,\u0026#34;\\n\u0026#34;) push!(Chain,U) U = [] rn = setdiff(soln,searched) ExtendU = rn[rand(1:end)] # don\u0026#39;t pick the same one twice.. push!(searched,ExtendU) stop=true # we\u0026#39;ve found a chain! end end end return(Chain) end We only stop searching when ExtendU is nothing because we know chains can only begin from an NDD and end with a pair who doesn\u0026rsquo;t give up a kidney (i.e. nothing). We return an array of our chains.\nCycles Similarly, we introduce a function to capture all of our cycles. This has many similarities to how we find cycles within our Recursive algorithm, with one noticeable difference being that we don\u0026rsquo;t search through pairs that were a part of a chain. We supply this function with an array of the chains we found in the above function, our solution from the recursive algorithm (y), our set of edges E, and a list of pairs in our solution soln.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 function find_cycles(foundchains,y,E,soln) fcarray = [] # find pairs that are a part of a chain Cycle = [] for i in 1:length(foundchains) union!(fcarray,foundchains[i]) end tosearch = setdiff(soln,fcarray) # search only pairs a part of cycles searched = [] U = [] ExtendU = tosearch[rand(1:end)] # start at ndd to find chain push!(searched,ExtendU) while length(setdiff(tosearch,searched))!=0 # while we haven\u0026#39;t searched all edges push!(U,ExtendU) ExtendU = nothing for j in tosearch if (in((U[end],j),E) \u0026amp;\u0026amp; y[(U[end],j)] \u0026gt; 0) ExtendU = j print(length(searched),\u0026#34;\\n\u0026#34;) push!(searched,ExtendU) break end end if U[1]==ExtendU # found a cycle push!(U,ExtendU) push!(Cycle,U) U = [] rn = setdiff(tosearch,searched) ExtendU = rn[rand(1:end)] push!(searched,ExtendU) end end push!(U,ExtendU) push!(Cycle,U) return(Cycle) end That\u0026rsquo;s about it! To finish, we call each of these functions and write the returned arrays into a .csv file. If you\u0026rsquo;d like to see the entire file and run a dataset on it for yourself, head over to my repository for the Julia file and the dataset. Happy programming!\nColumbia Surgery source on kidney transplants\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"May 17","permalink":"http://localhost:1313/projects/2020-05-17-kidney-exchange/","tags":null,"title":"Kidney Exchange in Julia"},{"categories":null,"contents":" Overview This project aims to implement and build a deeper level of understanding in Neural Networks. This article will profile how they learn just like the human brain does. Much of what you will see in this project is based on the first two chapters of the text by Michael Nielsen titled Neural Networks and Deep Learning. While Nielsen builds neural network that is capable of classifying handwritten digits in Python (2.7), I\u0026rsquo;ll show you how we can do it in R for a special sort of challenge.\nTo follow along or see the data, you can download from my repository on Github, which also includes the R script to load the data and build the Neural Network. What will follow will be two-fold: 1) a tl;dr version of what a Neural Network is and how it works; 2) an implementation in R for those who may want to learn how to do it in this language. For those unfamiliar with statistics, calculus, and data science, the first part of this article will be valuable to your understanding what Neural Networks are and how they work. That being said, the second part of this article should be valuable to those who are ready to dip their feet into the world of data science. With that out of the way, let\u0026rsquo;s get started.\nHow do Neural Networks, work? At a high level, Neural Networks are just that, a model of how your neurons work in your brain. The difference here is that it\u0026rsquo;s an emulation of your brain in a computer (not as scary as it sounds). The idea is that if you were shown the number 5 right now, your eyes would register the number, pass that information to your brain where certain neurons would fire based on the image. Then, your brain would determine that it is a 5 you are seeing.\nIn order to do this in a computer, we introduce a few equivalents to model what happens in the human brain. For this project, we are taking a large dataset of handwritten digits. Each digit comprises of 784 pixels, and looks like this:\nEach pixel represents an input, and each pixel is given a grayscale number, meaning a white pixel is 0, and a darker pixel is a number representing how dark the pixel is.\nSo, we input 784 values. These values are weighted (weights are learned through training data) and passed to the next layer of the network. A network and its layers look like this:\nEach of the 784 input values is sent to each of the nodes in the middle (hidden) layer. What I mean by this is that one pixel value is sent to each of the 30 nodes in the hidden layer. In our case, this represents a 784 by 30 matrix as you\u0026rsquo;ll see we use 30 nodes in the hidden layer. You\u0026rsquo;ll also notice 10 nodes in what is called the output layer. Each of these nodes represents a final determination of the handwritten digit being 0 through 9.\nLet\u0026rsquo;s talk through how one pixel (input) would pass through the entire network after having been weighted and passed to the middle (hidden) layer. From here, the new, weighted value is input into the middle layer. The node in the middle layer takes the value and runs it through what is called an activation function. In our case, we\u0026rsquo;ll use a Sigmoid function which looks like this:\nIn R, our now weighted input is passed into the below function as z:\n1 sigmoid \u0026lt;- function(z) 1/(1+exp(-z)) So what does the Activation (Sigmoid) function do? In laymen\u0026rsquo;s terms, it determines if the input is of value. You\u0026rsquo;ll see what this means in the next paragraph.\nFrom here, the output of the Sigmoid function is weighted and passed as input into the final layer of 10 nodes (remember: representing each of the 10 digits 0 through 9). That input is ran through the Activation function again, and the neural network outputs a vector of ten values, like this:\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; a [,1] [1,] 0.1041222329 [2,] 0.0056134030 [3,] 0.3600190030 [4,] 0.9930337436 [5,] 0.0004073771 [6,] 0.0179073440 [7,] 0.0938795106 [8,] 0.0071585077 [9,] 0.9863697174 [10,] 0.0175402033 These values represent how much the neural network \u0026ldquo;thinks\u0026rdquo; the handwritten inputted image is each number. We simply take the highest output (closest to 1) and consider the neural network to have classified the digit as that value. This output is from an untrained network, but it makes logical sense that the network thinks an 8 and 3 look similar. In this case, we\u0026rsquo;d say the network predicts that the handwritten image is a 3!\nThe key points to remember of how a network classifies a digit are: edges weight the inputs, nodes determine if those weighted inputs are of value, the values at the nodes are passed on to be weighted and valued again until the output layer is reached. The output tells us what the input should be classified as.\nImplementing NN in R Now that we\u0026rsquo;ve briefly walked through what steps a network takes to classify a handwritten digit, we must walk through how we train the network to get good at classifying digits correctly. Let\u0026rsquo;s try to do this while also showcasing some of the code to be implemented in R.\nTo train a network, we must give it some data so it can learn. We do this by splitting the dataset. If you\u0026rsquo;ve studied statistical modeling in any capacity, you\u0026rsquo;ll likely be familiar with this practice. In our handwritten image dataset we have 70,000 images, so we\u0026rsquo;ll feed our network 60,000 images to learn from and 10,000 to test on. The primary difference between the \u0026ldquo;learning\u0026rdquo; and \u0026ldquo;testing\u0026rdquo; digits is that in the learning phase we are able to adjust the weights and biases such that it gets more digits right. This is done through a reduction in what is called a cost function.\nStochastic Gradient Descent To start, we begin with the Stochastic Gradient Descent function. The cost function I referenced above? Gradient descent is a fancy way of saying we minimize the cost function (i.e. minimize how many digit classifications we get wrong). We want to minimize cost because cost represents how poorly our network classifies digits. The higher the cost, the worse our network classifies digits correctly.\nThe stochastic portion of Stochastic Gradient Descent references the fact that we are estimating gradient descent.\nWe estimate the gradient descent because in order to train our neural network, we will utilize a process called mini batching. We use mini batching for a number of reasons; the main reasons being that we can train our network using a much smaller amount of computing power (this is by far not a complete answer).\nTo begin building this learning part of our network, we must split the training data out into mini-batches of size 10 (meaning each mini batch has 10 handwritten images in it). In R, this is how we\u0026rsquo;ll do it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # appends the result to the 785th column, with 60000 # rows - one row per observation training_data \u0026lt;- cbind(train$x,train$y) for (j in 1:epochs){ # shuffle the data to prep for mini-batches training_data \u0026lt;- training_data[sample(nrow(training_data)),] mini.batches \u0026lt;- list() seq1 \u0026lt;- seq(from=1, to=60000, by=mini.batch.size) for(u in 1:(nrow(training_data)/mini.batch.size)){ # pull out 10 rows from training_data for each # iteration of this loop to create 6000 mini-batches mini.batches[[u]] \u0026lt;- training_data[seq1[u]:(seq1[u]+9),] } We create a nested list of 6000 mini batches, each of size mini.batch.size = 10.\nNow, we feed each mini batch through our neural network and calculate weights and biases such that it will classify the mini batch correctly. So, we build a for loop to iterate through each mini batch, and call a new function update.mini.batch.\nUpdate Mini Batch Within this function, we instantiate an empty list for the weights and biases. We iterate through each observation in the mini batch, feeding the gradient values (how dark a pixel is) as x and the actual handwritten digit (0 thru 9) as y.\n1 2 3 4 5 6 7 8 9 10 11 # create empty lists of weights and biases nabla.b \u0026lt;- list(rep(0,sizes[2]),rep(0,sizes[3])) nabla.w \u0026lt;- list(matrix(rep(0,(sizes[2]*sizes[1])), nrow=sizes[2], ncol=sizes[1]), matrix(rep(0,(sizes[3]*sizes[2])), nrow=sizes[3], ncol=sizes[2])) ## train through mini-batch for(p in 1:mini.batch.size){ x \u0026lt;- mini_batch[p,-785] # 784 input gradient values y \u0026lt;- mini_batch[p,785] # actual digit classification ## backprop for each observation in mini-batch delta_nablas \u0026lt;- backprop(x, y, sizes, num_layers, biases, weight) You\u0026rsquo;ll notice above we call our backpropagation function. Read on to see what this does.\nBackpropagation In laymens terms, backpropagation takes the observations of a mini batch and determines the weights and biases for each observation that will correctly classify the digits in the mini batch. That\u0026rsquo;s a mouth full, but this is how our network improves its classification ability. We will adjust our weights and biases for each observation in the mini batch. The reason it is called backpropagation is because through mathematical proofs, we can show how to efficiently calculate the gradient (or cost function) through each layer going backwards from the output layer. I will try to save the mathematical speak for Michael Nielsen, who explains it in detail here.\nTo implement in R, we initialize our weights and biases (remember: these are on the edges in the network) and a list of the activations at each node. To start, these activations are just our inputs (784 grey scale pixel values). We feed these values forward first, calculating what our current network would classify this digit as. All of this is done in the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## initialize updates nabla_b_backprop \u0026lt;- list(rep(0,sizes[2]),rep(0,sizes[3])) nabla_w_backprop \u0026lt;- list(matrix(rep(0,(sizes[2]*sizes[1])), nrow=sizes[2], ncol=sizes[1]), matrix(rep(0,(sizes[3]*sizes[2])), nrow=sizes[3], ncol=sizes[2])) ## Feed Forward activation \u0026lt;- matrix(x, nrow=length(x), ncol=1) # all 784 inputs in single column matrix activations \u0026lt;- list(matrix(x, nrow=length(x), ncol=1)) # list to store all activations, layer by layer zs \u0026lt;- list() # list to store all z vectors, layer by layer for(f in 1:length(weight)){ b \u0026lt;- biases[[f]] w \u0026lt;- weight[[f]] w_a \u0026lt;- w%*%activation b_broadcast \u0026lt;- matrix(b, nrow=dim(w_a)[1], ncol=dim(w_a)[2]) z \u0026lt;- w_a + b zs[[f]] \u0026lt;- z activation \u0026lt;- sigmoid(z) activations[[f+1]] \u0026lt;- activation } To help you understand where we are- we\u0026rsquo;ve just taken one observation; ran it through our network; calculated the weight on each edge; and calculated the activation at each node. Now is where we backpropagate. This means we determine the weights in which we will classify the digit correctly. We estimate this through the gradient of our cost function, meaning we attempt to minimize the chance our network missclassifies the digit.\n1 2 3 4 ## backpropagate where we update the gradient using delta errors delta \u0026lt;- cost.derivative(activations[[length(activations)]], y) * sigmoid_prime(zs[[length(zs)]]) nabla_b_backprop[[length(nabla_b_backprop)]] \u0026lt;- delta nabla_w_backprop[[length(nabla_w_backprop)]] \u0026lt;- delta %*% t(activations[[length(activations)-1]]) This calls our cost.derivative function. This function takes our vector of output activations (the list of 10 values between 0 and 1 from earlier), and subtracts 1 from the activation in which our digit actually is. This is important, as this is how our network learns. We take this vector of activations and calculate our output error (by multiplying by the derivative of our activation function).\n1 2 3 4 5 delta \u0026lt;- cost.derivative(activations[[length(activations)]], y) * sigmoid_prime(zs[[length(zs)]]) cost.derivative \u0026lt;- function(output.activations, y){ output.activations - digit.to.vector(y) } To close, we calculate our weights and biases that feed into our output layer, such that we get the digit classification right (or as close as we can get it). Based on these weights and biases, we can calculate the changes necessary to the weights and biases in the layer behind too. This, in essence, is backpropagation. We return a list of the weights and biases that, given the inputs we just gave the network, would classify the digit correctly. This is done in the code below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # take output from cost.derivative call and store it nabla_b_backprop[[length(nabla_b_backprop)]] \u0026lt;- delta nabla_w_backprop[[length(nabla_w_backprop)]] \u0026lt;- delta %*% t(activations[[length(activations)-1]]) # backpropagate through the layers behind the output for (q in 2:(num_layers-1)) { sp \u0026lt;- sigmoid_prime(zs[[length(zs)-(q-1)]]) delta \u0026lt;- (t(weight[[length(weight)-(q-2)]]) %*% delta) * sp nabla_b_backprop[[length(nabla_b_backprop)-(q-1)]] \u0026lt;- delta testyy \u0026lt;- t(activations[[length(activations)-q]]) nabla_w_backprop[[length(nabla_w_backprop)-(q-1)]] \u0026lt;- delta %*% testyy } return(list(nabla_b_backprop,nabla_w_backprop)) } The backpropagate function will be called each time as it iterates through each observation in the mini batch.\nFinish Update Mini Batch After we\u0026rsquo;ve calculated the weights and biases that would best classify each digit in our mini batch, we come back out of the backpropagation function and finish up updating our network. We take the weights and biases (of which we have a different set for each observation in the mini batch) and we edit the current weights and biases of the network. These edits are made based on what would be necessary to correctly classify the entire mini batch we just backpropagated, with a suppressing factor called the learning rate (or eta).\nTo touch on the learning rate briefly, it influences the extent of which we can change the current weights and biases of the network. This change is based on what would best classify the mini batch we just backpropagated. Without a learning rate, we may completely jump over our optimal weights and biases for which our network does the best at classifying all digits, not just the digits in a mini batch.\nTaking a step out\u0026hellip; So at a high-level, we\u0026rsquo;ve done the following thus far:\nSplit our training data into batches of 10 Backpropagated to determine the weights and biases for which our network would best classify each handwritten digit in the mini batch correctly Updated the weights and biases of our network to best classify based on the mini batch it has just been trained on To finish training the network, we simply have to set up a for loop to do everything we\u0026rsquo;ve talked about up to this point. For each mini batch, we update the weights and biases of the network to better classify digits. Remember, we had a dataset of 60,000 digits, with batch sizes of only 10 digits, so we\u0026rsquo;ll iterate many times. Once we\u0026rsquo;ve done that, we can evaluate our network on our testing data. If we\u0026rsquo;ve done everything right, we should have well tuned weights and biases that should classify images of handwritten digits at 50% accuracy.\nEpochs You read that right! 50% accuracy. That\u0026rsquo;s because I forgot to mention, once we\u0026rsquo;ve tuned the weights and biases over each mini batch we\u0026rsquo;ve only completed one epoch. Remember the learning rate I talked about earlier? Instead of loosening things up and running the risk of our network getting worse over time, we have to do an epoch a number of times so our network can incrementally improve towards optimality. This means we keep the weights and biases of the network, randomly split our data into mini batches, backpropagate over each mini batch, and update weights all over again.\nIdeally, once we\u0026rsquo;ve tuned our network\u0026rsquo;s weights over a number of epochs, we could begin using our network to classify digits in real time. Think along the lines of banks automating the processing of checks. Kind of cool, right?\nIn Closing If you\u0026rsquo;ve made it this far, congratulations. I hope you\u0026rsquo;ve learned a little bit about how neural networks are implemented and how they learn. If you\u0026rsquo;d like to try it our for yourself, see the source code. There are a number of helper functions that I didn\u0026rsquo;t go over for the sake of brevity, so be sure to familiarize yourself with those too. Otherwise, leave a comment and let me know what you thought of this project!\n","date":"May 11","permalink":"http://localhost:1313/projects/2020-05-11-neuralnetwork/","tags":null,"title":"Neural Networks in R"},{"categories":null,"contents":"I recently took the opportunity to speak to clients of Analytics8 about keeping their data transformation practices up to date through periodic health checks!\n","date":"Mar 21","permalink":"http://localhost:1313/post/dbt-healthcheck-webinar/","tags":null,"title":"5 Steps to Optimize your dbt Implementation"},{"categories":null,"contents":"","date":"Feb 19","permalink":"http://localhost:1313/post/austin-marathon/","tags":null,"title":"Recap: My First Marathon"},{"categories":null,"contents":"About five months ago, I started my first training block ever to run a long distance race.\nI think it is a common experience for many who grow up playing team sports to view running as a punishment. Missed your free throws? Run lines. Got blown out on your home field? Next practice, we\u0026rsquo;re running. The crazy thing is, I even ran cross country for a time growing up, but even then, that was about running a race to beat people.\nA weird thing happens as you get older. They say you have the vast majority of your novel experiences in the first 25 years of your life. After that, time starts to move pretty quickly because you are simply just used to the world around you. It is with this truth that finding enjoyment in the mundane becomes a focus. Running, for me, has become one of those mundanities. There is something about the steady pounding of the pavement to a beat that just puts you into a meditative state. The day\u0026rsquo;s stresses fade away for the time being, and the only thing that truly matters is taking that next step.\nI originally set out to run this race as my first long distance event, which it was, but I did not anticipate feeling good enough to run 13 miles in a single session about a month into what would be a just shy of a five month training block (I was starting from zero, thus the extended length of the block!). As is common in the marathoning business, this race became earmarked as a tuneup of sorts instead, a marker on my progress towards what would become my new goal, running my first full marathon.\nThe progress has been steady, but in the thick of a training block where I have been quickly adapting to faster paces and longer distances (faster than anticipated), going into this race I really had no idea what my potential might be. The 3M Half Marathon in Austin, Texas is well known as a fast course. Starting north of the city and heading into the river valley downtown, this course gives a nice net drop in elevation leading to a lot of PRs. With weather in the 40s, cloudy and a light breeze it was the perfect conditions to put up some fast splits.\nWanting to run a negative split race, I started out just ahead of the 1:45 pace group, thinking 1:40 would be a great finish for me based on my training leading up to the race. Perhaps what I didn\u0026rsquo;t anticipate is the power that on-course energy, adrenaline of a race, and the motivation to catch and pass pacers would carry me further than I thought.\nBy mile 5, I had caught the 1:40 pace group and we were approaching a familiar part of the course to me, Great Northern Boulevard, where I had done training runs. I had not done a great job tapering leading up to this race, having ran the Thursday prior to this Sunday race, but any remaining soreness from the slog of my training block had worn off and I was feeling in a groove. I chose to settle in and get comfortable in the middle miles. Thinking I would not catch the 1:35 group, I let the pace slip slightly from 7 minute miles as we started hitting some rolling hills.\nIt\u0026rsquo;s probably around mile 10 that I can see the 1:35 stick in the distance as we go up and down 45th. As we turned south I knew I had friends waiting around mile 11 to cheer me on, having not even seen them yet I was already drawing an energy boost from it in anticipation. By the time I had reached them, I was just behind the 1:35 pacer, with the group starting to take off ahead for a fast finish. I followed suit, turning down through UT\u0026rsquo;s campus (another familiar place from training runs), I could feel the impending finish line and started to empty the tank. With a couple tough uphill sections, I was able to turn in a 7 flat and a 6:50 for my final two splits.\nWhat blows my mind still is that over the course of my training I don\u0026rsquo;t think I turned in a single mile at a 7 minute pace. I did weekly speed/interval workouts (sometimes with hills), but never held a pace like that for a mile, maybe half that for a single repetition. It is still very counterintuitive to me that building weekly mileage at slower paces still sets you up for fast races like this. Conventional wisdom- or maybe common sense, might suggest that you actually run your goal paces during workouts. I have done so in anticipation of my goal marathon pace, but was blown away by my endurance at a faster pace over a shorter distance.\nCompleting this race has been a huge motivator to knock out my peak 50 mile week leading up to a taper for the Austin Marathon next month. While I will need to be conservative due to the fast nature of this course in predicting marathon distance times, I have absolutely had to adjust my goals for racing the marathon distance. Austin is notoriously a hilly (no, Texas is not all flat!) course, with an unrelenting climb north from miles 11 to 18 which will put my hill training sessions to the test, but this race has served its role as a barometer of my level of fitness nonetheless.\nWhen my first marathon is in the books I will crack open the training data captured on each run via my watch and do some discovery and analysis over this entire block. I may even spin up and host a dashboard for people to play around with, so check back with me soon!\nStrava clout below:\n","date":"Jan 23","permalink":"http://localhost:1313/post/3m-half/","tags":null,"title":"I ran my first half-marathon!"},{"categories":null,"contents":"I recently spoke at dbt labs\u0026rsquo; Coalesce Conference in San Diego! Alongside the Head of Engineering for the Roche Go-to-market Analytics Department, Yannick Misteli and I discuss multi-project deployments of dbt to scale your analytics workflow. This talk precedes features that were announced at this Coalesce Conference around dbt mesh, discussing how Roche managed model interaction between dbt projects before these features existed.\nYou can watch a recording of the talk below:\nIf you work for an analytics team thinking about adopting a multi-project architecture with dbt, I\u0026rsquo;m always happy to discuss your vision with you and offer insights having done this at scale. Comment below or reach out to me via tyler [at] tylerrouze.com!\n","date":"Dec 08","permalink":"http://localhost:1313/post/coalesce-2023/","tags":null,"title":"Coalesce Conference 2023: How Roche GTM Manages Multi-project dbt Deployments"},{"categories":null,"contents":"I recently made my first dbt-core open-source contribution! This contribution adds unmodified and old state selection methods to the dbt CLI. The main motivation is to make it easy to exclude late-binding views from our dbt job\u0026rsquo;s DAG, reducing the runtime overhead. You can see some details that precipitated this contribution here.\nYou can watch a short demo on the contribution below:\nTo take it for a spin yourself, you might run a command that excludes the intersection of views and unchanged models from the DAG:\n1 dbt run -s config.materialized:view,state:unmodified --state ./path/to/artifats If you end up using this feature extension, I\u0026rsquo;d love to hear from you! When I worked on this, I did it entirely inspired by the late-binding view use-case, but I\u0026rsquo;d be curious to hear how you put it to use in your dbt jobs. As always, reach out to me via tyler [at] tylerrouze.com or comment below!\n","date":"Aug 03","permalink":"http://localhost:1313/post/first-dbt-core-contribution/","tags":null,"title":"My First dbt-core Contribution!"},{"categories":null,"contents":"I took the opportunity to share some insights about my experiences in scaling dbt development in the enterprise during the May 2023 dbt Chicago Meetup. You can see the slides below and picture of me speaking at the event below!\nPrevious Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","date":"May 01","permalink":"http://localhost:1313/post/dbt-meetup-chicago-may-23/","tags":null,"title":"dbt Chicago Meetup - May 2023"},{"categories":null,"contents":"","date":"Feb 08","permalink":"http://localhost:1313/post/dbt-with-databricks/","tags":null,"title":"Q\u0026A Session: What Does dbt Add to Databricks?"},{"categories":null,"contents":"","date":"Jul 01","permalink":"http://localhost:1313/post/a8-dbt-webinar/","tags":null,"title":"How dbt Helps Our Clients Remove Data Engineering Bottlenecks"},{"categories":null,"contents":" This page lists the most up-to-date information regarding what I am doing right now.\nA little bit about me. Motorhead, flying, golfing, baseball, hiking, skiing, bread making, running, you name it. My passions carry me in many different and odd directions, but I wouldn\u0026rsquo;t have it any other way.\nLife I currently live in Austin, TX.\nWork I currently work for Analytics8 as a Managing Consultant. My role focuses on modernizing clients\u0026rsquo; data architectures, data modeling practices, and developer workflows while managing a team of consultants working on similar projects. I am a certified professional in a number of technologies, including dbt, Google Cloud, and Databricks. I have even made open-source contributions to the dbt-core project!\nEducation I have quite a passion for lifelong learning. So while much of education is in the past, I am definitely continuing to learn new topics right /now. It\u0026rsquo;s this passion that motivated me to learn web development by making this website. More formally, I graduated from the University of Minnesota with a degree in Industrial and Systems Engineering (yes that\u0026rsquo;s me on the front page!). If you don\u0026rsquo;t know what the IE curriculum entails, it is the science of Decision making, usually based on data, done so via:\nStatistics Optimization Simulation Thanks to my time in college, I have strong passions for data-driven solutions and using technology to eliminate repetitive tasks and free up time for more creative tasks.\nCurrent Interests Some of my current interests are:\nFormula 1 Running (Chicago Marathon on deck!) Aviation (Microsoft Flight Simulator anyone?) Any book on our Human Nature (psychology/sociology), especially those with data driven research findings. Modern Data Stack (especially dbt) If you share some of the same interests, drop a line below!\nHow can I get in touch? Send me an email to tyler @ this domain. If you simply want to keep up with what\u0026rsquo;s going on with me, you can subscribe to the mailing list for updates delivered straight to your inbox.\nThis page is an inspiration from this article\n","date":"Jan 01","permalink":"http://localhost:1313/now/","tags":null,"title":"/now"},{"categories":null,"contents":"","date":"Jan 01","permalink":"http://localhost:1313/posts/","tags":null,"title":"Posts"},{"categories":null,"contents":"\n","date":"Jan 01","permalink":"http://localhost:1313/socials/","tags":null,"title":"Social Links"},{"categories":null,"contents":" Additionally, if you\u0026rsquo;d like to get in touch via email, send me a note to tyler @ this domain.\n","date":"Jan 01","permalink":"http://localhost:1313/subscribe/","tags":null,"title":"Subscribe"},{"categories":null,"contents":" You\u0026rsquo;ll be kept up to date with future events. In the meantime, if you haven\u0026rsquo;t already, check out what else I\u0026rsquo;ve been doing.\n","date":"Jan 01","permalink":"http://localhost:1313/thanks/","tags":null,"title":"Thank you"}]